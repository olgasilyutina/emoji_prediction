{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>emopok data pipeline ü§î</center></h1>\n",
    "<center>authors: [Aina Nurmagombetova](https://github.com/anurma) ü§ô [Alina Cherepanova](https://github.com/alinacherepanova) üôã [Anya Bataeva](https://github.com/fyzbt) ü§Ø [Olya Silyutina](https://github.com/olgasilyutina) ü§©</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emopok\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading df with twitter and telegram data\n",
    "df = pd.read_csv('./data/new_df_twi_tg.csv')\n",
    "df['new_id'] = df.groupby(df.texts.tolist(), sort=False).ngroup() + 1\n",
    "df_subset = df[['texts', 'new_id']]\n",
    "unique_df = df_subset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['texts', 'emoji', 'new_id']].to_csv(\"./data/emoji_texts_df.csv\", header=['texts', 'emoji', 'index'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"./data/unique_emopok.csv\", header=['texts', 'index'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing and writing into csv file\n",
    "emopok.preprocess_text(unique_df['texts'], unique_df['new_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('./data/clean_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üò≠ sentiments üòÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pictures/socialsent.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[William L. Hamilton, Kevin Clark, Jure Leskovec, and Dan Jurafsky. Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora. Proceedings of EMNLP. 2016. (to appear; arXiv:1606.02820).](https://github.com/williamleif/socialsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5191247a97d44f94a99cb3b9507fb00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=357077), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create list of sentences for word2vec model\n",
    "w2v_sentences = [nltk.word_tokenize(str(i)) for i in tqdm(clean_data['clean_texts'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model and setting values for the various parameters\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 5 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 5       # Context window size\n",
    "iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = emopok.train_word2vec(w2v_sentences, num_workers, num_features, min_word_count, context, iterations, \\\n",
    "                                  file_path = './models/emopok_w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"./models/emopok_w2v_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('–ø—Ä–∏–≤–µ—Ç–∏–∫', 0.7054356336593628),\n",
       " ('–∞–ø—Ä–µ–ª—å–∫–∞', 0.49257901310920715),\n",
       " ('–ø—Ä–∏–≤–µ–µ–µ—Ç', 0.4709334373474121),\n",
       " ('–¥–µ–ª–∏—à–∫–∏', 0.4653734862804413),\n",
       " ('—Ç–µ–∑–∫–∞', 0.4564298689365387),\n",
       " ('–≤–µ—Å—Ç–æ—á–∫–∞', 0.4510449171066284),\n",
       " ('—Å–µ—Å—Ç—Ä–∏—á–∫–∞', 0.4470579922199249),\n",
       " ('—à–º—É–ª—å', 0.4408361613750458),\n",
       " ('–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ', 0.4390951693058014),\n",
       " ('–¥–µ–Ω–∏—Å–æ–≤–Ω–∞', 0.43712013959884644)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similar_by_word('–ø—Ä–∏–≤–µ—Ç')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats on word counts\n",
    "flat_sentences = [item for sublist in w2v_sentences for item in sublist]\n",
    "words_count = Counter(flat_sentences)\n",
    "words_count = pd.DataFrame.from_dict(words_count, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cefe762e7b4803babb7625277cec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=135713), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get input file for socialsent\n",
    "words_count.columns = ['lem_word', 'count']\n",
    "words = []\n",
    "vectors = []\n",
    "for word in tqdm(words_count['lem_word']):\n",
    "    try:\n",
    "        vectors.append(list(w2v_model[word]))\n",
    "        words.append(word)\n",
    "    except:\n",
    "        Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec output to txt file for socialsent\n",
    "# script for sentiment analysis is here https://github.com/olgasilyutina/socialsent3/blob/master/example.ipynb\n",
    "data_vect = pd.DataFrame(vectors, columns=list(range(num_features)))\n",
    "data_vect.index = words\n",
    "data_vect['lem_word'] = words\n",
    "data_vect = data_vect.drop('lem_word', axis = 1).reset_index()\n",
    "data_vect.to_csv('./data/data_emopok.txt', header = None, index = None, sep = ' ', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results of socialsent model\n",
    "sent_dfs = []\n",
    "\n",
    "for file in glob.glob(\"./data/polarities/*.json\"):\n",
    "    with open( file) as f:\n",
    "        data = json.load(f)\n",
    "    sent_dfs.append(pd.DataFrame(data, index=[0]).T.reset_index())\n",
    "\n",
    "sent_df = pd.concat(sent_dfs)\n",
    "sent_df.columns = ['word', 'sent']\n",
    "sent_df = sent_df.reset_index().drop('index', axis=1).drop_duplicates()\n",
    "sent_df = sent_df[sent_df['sent'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcelate sentiments for every text\n",
    "df_sent = pd.DataFrame({'doc': w2v_sentences})\n",
    "df_sent = df_sent.reset_index()\n",
    "df_sent['index'] = df_sent['index'] + 1\n",
    "df_sent = df_sent.set_index(['index'])['doc'].apply(pd.Series).stack()\n",
    "df_sent = df_sent.reset_index()\n",
    "df_sent = df_sent.drop('level_1', axis=1)\n",
    "df_sent.columns = ['index', 'word']\n",
    "df_sent = df_sent.merge(sent_df, on=['word'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.DataFrame(df_sent.groupby('index').sent.sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent.columns = ['new_id', 'sent']\n",
    "df_sent_texts = df_sent.merge(unique_df, on = 'new_id')\n",
    "pd.merge(df_sent, unique_df, how='inner', left_on=['index'], right_on=['new_id'])\n",
    "df_sent_texts.to_csv('./data/sentiments_emopok.csv', header=['index', 'sent', 'texts'], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑ text features üë∑‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11039dcb31b4a9f8620b8bbeadb6498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get text features\n",
    "textfeatures_df = []\n",
    "for text, index in tqdm(zip(unique_df['texts'], unique_df['new_id'])):\n",
    "    textfeatures_df.append(emopok.textfeatures(text, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfeatures_df = pd.concat(textfeatures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_commas</th>\n",
       "      <th>n_digits</th>\n",
       "      <th>n_exclaims</th>\n",
       "      <th>n_hashtags</th>\n",
       "      <th>n_lowers</th>\n",
       "      <th>n_mentions</th>\n",
       "      <th>n_urls</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_nonasciis</th>\n",
       "      <th>n_uppers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "      <td>357077.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178539.000000</td>\n",
       "      <td>70.611608</td>\n",
       "      <td>0.657847</td>\n",
       "      <td>1.122324</td>\n",
       "      <td>0.183358</td>\n",
       "      <td>0.053823</td>\n",
       "      <td>50.520840</td>\n",
       "      <td>0.604973</td>\n",
       "      <td>0.179992</td>\n",
       "      <td>26.987843</td>\n",
       "      <td>45.531244</td>\n",
       "      <td>4.089048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>103079.395373</td>\n",
       "      <td>74.893091</td>\n",
       "      <td>1.272483</td>\n",
       "      <td>5.267934</td>\n",
       "      <td>0.864982</td>\n",
       "      <td>0.339021</td>\n",
       "      <td>53.307267</td>\n",
       "      <td>1.564861</td>\n",
       "      <td>0.446786</td>\n",
       "      <td>36.221020</td>\n",
       "      <td>51.563273</td>\n",
       "      <td>8.583359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>89270.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>178539.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>267808.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>357077.000000</td>\n",
       "      <td>4056.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>1492.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3154.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3474.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>511.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index        n_chars       n_commas       n_digits  \\\n",
       "count  357077.000000  357077.000000  357077.000000  357077.000000   \n",
       "mean   178539.000000      70.611608       0.657847       1.122324   \n",
       "std    103079.395373      74.893091       1.272483       5.267934   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%     89270.000000      26.000000       0.000000       0.000000   \n",
       "50%    178539.000000      49.000000       0.000000       0.000000   \n",
       "75%    267808.000000      90.000000       1.000000       1.000000   \n",
       "max    357077.000000    4056.000000     115.000000    1492.000000   \n",
       "\n",
       "          n_exclaims     n_hashtags       n_lowers     n_mentions  \\\n",
       "count  357077.000000  357077.000000  357077.000000  357077.000000   \n",
       "mean        0.183358       0.053823      50.520840       0.604973   \n",
       "std         0.864982       0.339021      53.307267       1.564861   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000      18.000000       0.000000   \n",
       "50%         0.000000       0.000000      34.000000       0.000000   \n",
       "75%         0.000000       0.000000      65.000000       1.000000   \n",
       "max        62.000000      21.000000    3154.000000      50.000000   \n",
       "\n",
       "              n_urls        n_words    n_nonasciis       n_uppers  \n",
       "count  357077.000000  357077.000000  357077.000000  357077.000000  \n",
       "mean        0.179992      26.987843      45.531244       4.089048  \n",
       "std         0.446786      36.221020      51.563273       8.583359  \n",
       "min         0.000000       1.000000       0.000000       0.000000  \n",
       "25%         0.000000       8.000000      14.000000       1.000000  \n",
       "50%         0.000000      20.000000      29.000000       2.000000  \n",
       "75%         0.000000      35.000000      58.000000       4.000000  \n",
       "max        23.000000    3474.000000    3276.000000     511.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfeatures_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfeatures_df.to_csv('./data/textfeatures_emopok.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëâ doc2vec üëà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pictures/doc2vec.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quoc Le, Tomas Mikolov Distributed Representations of Sentences and Documents, Proceedings of the 31 st International Conference on Machine\n",
    "Learning. 2014](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text doc2vec representations\n",
    "preprocess_text(unique_df['texts'], unique_df['new_id'], lemmatize = True, stopwords = False, \\\n",
    "                russian_only = False, file_path = './data/d2v_clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_clean_data = pd.read_csv('./data/d2v_clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3985276d3b2c421ca9001497cc0aea22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=357077), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create list of sentences for doc2vec model\n",
    "d2v_sentences = [nltk.word_tokenize(str(i)) for i in tqdm(d2v_clean_data['clean_texts'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2f67769de74caba4de10fe2218d938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was successfully saved with a name  ./models/emopok_w2v_model\n"
     ]
    }
   ],
   "source": [
    "d2v_model, all_vectors = emopok.train_doc2vec(d2v_sentences, 100, 5, 10, 5, save_model_to = './models/emopok_d2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors_df = pd.DataFrame(all_vectors)\n",
    "all_vectors_df.columns = ['d2v_' + str(col) for col in all_vectors_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors_df.to_csv('./data/d2v_vectors_emopok.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ LDA üëÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pictures/lda.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Blei D. M., Ng A. Y., Jordan M. I. Latent dirichlet allocation //Journal of machine Learning research. ‚Äì 2003. ‚Äì –¢. 3. ‚Äì ‚Ññ. Jan. ‚Äì –°. 993-1022.](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "sentences = w2v_sentences\n",
    "corpus = emopok.get_lda_model(sentences, num_topics, file_path = './models/emopok_lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_emo = emopok.load(\"./models/emopok_lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = emopok.get_topics_for_docs(corpus, lda_model_emo, 20, unique_df['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.read_csv('./data/topics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_topics = pd.get_dummies(topics_df['topic'])\n",
    "dum_topics.columns = ['topic_' + str(col) for col in dum_topics.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_topics.to_csv('./data/dum_topics_emopok.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåà emoji clusterization üåà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pictures/tsne.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed2cea8323f41229ae9bc9ff86dc63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=357077), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emoji_texts = []\n",
    "unique_emojies = list(df.emoji.unique())\n",
    "\n",
    "for text in tqdm(unique_df['texts']):\n",
    "    emoji_texts.append(emopok.get_emoji_sentences(text, unique_emojies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was successfully saved with a name  ./models/emopok_w2v_emoji_model\n"
     ]
    }
   ],
   "source": [
    "# creating the model and setting values for the various parameters\n",
    "num_features = 1000  # Word vector dimensionality\n",
    "min_word_count = 50 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 2       # Context window size\n",
    "iterations = 20\n",
    "\n",
    "w2v_emoji_model = emopok.train_word2vec(emoji_texts, num_workers, num_features, min_word_count, context, iterations, \\\n",
    "                           file_path = './models/emopok_w2v_emoji_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emoji_model = Word2Vec.load(\"./models/emopok_w2v_emoji_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üíù', 0.8745415210723877),\n",
       " ('üíü', 0.8650382161140442),\n",
       " ('üíò', 0.8625384569168091),\n",
       " ('üíì', 0.8616468906402588),\n",
       " ('üíñ', 0.830069899559021),\n",
       " ('‚ù£', 0.8249319791793823),\n",
       " ('üíó', 0.8223015069961548),\n",
       " ('üíï', 0.8110256195068359),\n",
       " ('üíå', 0.787800669670105),\n",
       " ('üíã', 0.7590600252151489)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_emoji_model.similar_by_word('üíû')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emojies = df.groupby('emoji').texts.count().reset_index().sort_values('texts')\n",
    "unique_emojies = unique_emojies[unique_emojies['texts'] >= 50].emoji.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emojis_found = [e for e in unique_emojies if e in w2v_emoji_model.wv.vocab]\n",
    "X = [w2v_emoji_model.wv[e] for e in unique_emojies if e in w2v_emoji_model.wv.vocab]\n",
    "\n",
    "emopok.search_for_kmeans(30, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters = emopok.train_kmeans(26, X, emojis_found, save_to = './data/emopok_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster 15 and 16 clusters separately\n",
    "emo_clusters = pd.read_csv('./data/emopok_clusters.csv')\n",
    "emo_clusters = emo_clusters.astype(str)\n",
    "cluster = '15'\n",
    "unique_emojies = emo_clusters[emo_clusters.cluster_group == cluster]['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_found = [e for e in unique_emojies if e in w2v_emoji_model.wv.vocab]\n",
    "X = [w2v_emoji_model.wv[e] for e in unique_emojies if e in w2v_emoji_model.wv.vocab]\n",
    "\n",
    "emopok.search_for_kmeans(15, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters = emopok.train_kmeans(12, X, emojis_found, save_to = f'./data/emopok_clusters_{cluster}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in list(emo_clusters[['cluster_group']].drop_duplicates()['cluster_group']):\n",
    "    print(emo_clusters[emo_clusters.cluster_group == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters = pd.read_csv('./data/emopok_clusters.csv')\n",
    "emo_clusters = emo_clusters[~emo_clusters['cluster_group'].isin([15, 16])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters_15 = pd.read_csv('./data/emopok_clusters_15.csv')\n",
    "emo_clusters_15['cluster_group'] = '15_' + emo_clusters_15['cluster_group'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters_16 = pd.read_csv('./data/emopok_clusters_16.csv')\n",
    "emo_clusters_16['cluster_group'] = '16_' + emo_clusters_16['cluster_group'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters = pd.concat([emo_clusters, emo_clusters_15, emo_clusters_16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_clusters.to_csv('./data/emopok_clusters.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emo_clusters.groupby('cluster_group').count().reset_index().sort_values('index', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
